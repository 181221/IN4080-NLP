{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080, 2019, Mandatory assignment B\n",
    "**This is part B. \n",
    "Do part A first.**\n",
    "\n",
    "See part A for delivery date and general requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will experiment with sequence classification and tagging. We will combine some of the tools for tagging from NLTK with scikit-learn to build various taggers.We will start with simple examples from NLTK where the tagger only considers the token to be tagged---not its context---and work towards more advanced logistic regression taggers (also called maximum entropy taggers). Finally,  we will compare to some tagging algorithms installed in NLTK.\n",
    "\n",
    "In this set you will get more experience with\n",
    "\n",
    "- baseline for a tagger\n",
    "- how different tag sets may result in different accuracies\n",
    "- feature selection\n",
    "- the effect of the machine learner\n",
    "- smoothing \n",
    "- evaluation\n",
    "- in-domain and out-of-domain evaluation\n",
    "\n",
    "To get a good tagger, you need a reasonably sized training corpus. Ideally, we would have used the complete Brown corpus in this exercise, but it turns out that some of the experiments we will run, will be time consuming. Hence, we will follow the NLTK book and use only the News section. Since this is a rather homogeneous domain, and we also pick our test data from the same domain, we can still get decent results.\n",
    "\n",
    "Towards the end of the exercise set, we will see what happens if we take our best setting from the News section to a bigger domain.\n",
    "\n",
    "Beware that even with this reduced corpus, some of the experiments will take several minutes. And when we build the full tagger in exercise 5, an experiment may take more than an hour. So make sure you start the work early enough. (You might do other things while the experiments run.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating NLTK Ch. 6 \n",
    "\n",
    "We jump into the NLTK book, chapter 6, the sections 6.1.5 Exploiting context and 6.1.6 Sequence classification. You are advised to read them before you start.\n",
    "\n",
    "We start by importing NLTK and the tagged sentences from the news-section from Brown, similarly to the NLTK book.\n",
    "\n",
    "Then we split the set of sentences into a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sklearn\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like NLTK, our tagger will have two parts, a feature extractor, here called **pos_features**, and a general class for building taggers, **ConsecutivePosTagger**.\n",
    "\n",
    "We have made a few adjustments to the NLTK setup. We are using the *pos_features* from section 6.1.5 together with the *ConsecutivePosTagger* from section 6.1.6. The *pos_features* in section 1.5 does not consider history, but to get a format that works together with *ConsecutivePosTagger*, we have included an argument for history in *pos_features*, which is not used initially. (It get used by the *pos_features* in section 6.1.6 of the NLTK book, and you may choos to use it later in this set).\n",
    "\n",
    "Secondly, we have made the *feature_extractor* a parameter to *ConsecutivePosTagger*, so that it can easily be replaced by other feature extractors while keeping *ConsecutivePosTagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " def pos_features(sentence, i, history): \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the NLTK bok, we train and test a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915\n"
     ]
    }
   ],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give results comparable to the NLTK book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: initial experiment and baseline (10 points)\n",
    "### Part a. Some simple refinements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown tags that come with NLTK are extended compared to the original tag set. \n",
    "To get better results from our tagger, we strip off the part after the hyphen and stick to the original Brown tags, a tagset of 87 tags. \n",
    "(This is somewhat simplified, cf., weekly exercise set 2.) \n",
    "We can  repeat the training and testing, and we see a slightly improved result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314\n"
     ]
    }
   ],
   "source": [
    "def originize(tagged_sents):\n",
    "    \"\"\"Change tags to original Brown tags in tagged_sents\"\"\"\n",
    "    return [ [(word, tag.split('-')[0]) for (word,tag) in sent]\n",
    "             for sent in tagged_sents]\n",
    "\n",
    "orig_train_sents = originize(train_sents)\n",
    "orig_test_sents = originize(test_sents)\n",
    "\n",
    "orig_tagger_1 = ConsecutivePosTagger(orig_train_sents)\n",
    "print(round(orig_tagger_1.evaluate(orig_test_sents), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these original Brown tags for the rest of this exercise set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be a little more cautious than the NLTK-book, when it comes to training and test sets. We will split the News-section into three sets\n",
    "\n",
    "- 10% for final testing which we tuck aside for now, call it *news_test*\n",
    "- 10% for development testing, call it *news_dev_test*\n",
    "- 80% for training, call it *news_train*\n",
    "\n",
    "And we will use the original Brown tags as explained.\n",
    "\n",
    "- Make the data sets, and repeat the training and evaluation with *news_train* and *news_dev_test*.\n",
    "- Please use 4 counting decimal places and stick to that throughout the exercise set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8007\n"
     ]
    }
   ],
   "source": [
    "complete_set = train_sents + test_sents\n",
    "news_train, news_dev_test, news_test = np.split(complete_set, [int(.8*len(complete_set)), int(.9*len(complete_set))])\n",
    "\n",
    "news_train = originize(news_train)\n",
    "news_dev_test = originize(news_dev_test)\n",
    "news_test = originize(news_dev_test)\n",
    "\n",
    "conpos_tagger = ConsecutivePosTagger(news_train)\n",
    "print(round(conpos_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b. Baseline \n",
    "\n",
    "One of the first things we should do in an experiment like this, is to establish a reasonable baseline. A reasonable baseline here is the Most Frequent Class baseline.\n",
    "Each word which is seen during training should get its most frequent tag from the training. \n",
    "For words not seen during training, we simply use the most frequent overall tag.\n",
    "\n",
    "With news_train as training set and news_dev_set as valuation set, what is the accuracy of this baseline?\n",
    "\n",
    "Does he NLTK-tagger beat the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "class BaselinePosTagger(nltk.TaggerI): \n",
    "    def __init__(self, train_sents):\n",
    "        self.train_sents = train_sents\n",
    "        self.cfd = ConditionalFreqDist([(word.lower(),tag) for sentence in train_sents for (word,tag) in sentence])\n",
    "        self.max_ = 0\n",
    "        self.most_common_tag = ''\n",
    "        self.most_common_pos()\n",
    "        \n",
    "    def most_common_pos(self):\n",
    "        tags = {}\n",
    "        max_  = 0\n",
    "        max_word = {}\n",
    "        for sentence in self.train_sents:\n",
    "            for word, tag in sentence:\n",
    "                tags[word] = tag\n",
    "        for key in self.cfd:\n",
    "            if self.cfd[key].N() > max_:\n",
    "                max_ = self.cfd[key].N()\n",
    "                max_word[max_] = key\n",
    "        self.max = max_\n",
    "        self.most_common_tag = tags[max_word[max_]]\n",
    "        \n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            if self.cfd[word].N() > 0:\n",
    "                history.append(self.cfd[word].max())\n",
    "            else:\n",
    "                history.append(self.most_common_tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7434\n"
     ]
    }
   ],
   "source": [
    "tagger = BaselinePosTagger(news_train)\n",
    "print(round(tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK-tagger beats my baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: scikit-learn and tuning (10 points)\n",
    "Our goal will be to improve the tagger compared to the simple suffix-based tagger. For the further experiments, we move to scikit-learn which yields more options for considering various alternatives. We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNB-classifier which should correspond to the way it is done in NLTK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, \n",
    "                 features=pos_features, clf = BernoulliNB(alpha=0.5)):\n",
    "        # Using pos_features as default.\n",
    "        # Using BernoulliNB() (with alpha/lidstone 0.5)\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "    def get_features(self):\n",
    "        return self.dict.vec.get_feature_names()\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a.\n",
    "Train the ScikitConsecutivePosTagger on the *news_train* set and test on the *news_dev_test* set with the *pos_features*. Do you get the same result as with the original NLTK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7258\n"
     ]
    }
   ],
   "source": [
    "SciKit_tagger = ScikitConsecutivePosTagger(news_train)\n",
    "print(round(SciKit_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "I get inferior results compared to using the NLTK set-up with the same feature extractors. The only explanation I could find is that the smoothing is too strong. Therefore, try again with alpha in [1, 0.5, 0.1, 0.01, 0.001, 0.0001]. What do you find to be the best value for alpha?\n",
    "\n",
    "With the best choice of alpha, do you get the same results as with NLTK, worse results or better results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.8039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.8102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.8095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.8116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha\n",
       "1.0000  0.6476\n",
       "0.5000  0.7258\n",
       "0.1000  0.8039\n",
       "0.0100  0.8102\n",
       "0.0010  0.8095\n",
       "0.0001  0.8116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "lst = []\n",
    "for alpha in alphas:\n",
    "    SciKit_tagger = ScikitConsecutivePosTagger(news_train,features=pos_features, clf = BernoulliNB(alpha=alpha))\n",
    "    lst.append(round(SciKit_tagger.evaluate(news_dev_test), 4))\n",
    "df = pd.DataFrame(lst, index=alphas,columns = ['alpha'])\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c.\n",
    "To improve the results we may change the feature selector or the machine learner. We start with a simple improvement of the feature selector. The NLTK selector considers the previous word, but not the word itself. Intuitively, the word itself should be a stronger feature. Extend the NLTK feature selector with a feature for the token to be tagged. Rerun the experiment with various alphas and record the results. Which alpha gives the best accuracy and what is the accuracy?\n",
    "\n",
    "Did the extended feature selector beat the baseline? Intuitively, it should get as least as good accuracy as the baseline. Explain why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.8631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.8765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.8822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.8882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha\n",
       "1.0000  0.6353\n",
       "0.5000  0.7524\n",
       "0.1000  0.8631\n",
       "0.0100  0.8765\n",
       "0.0010  0.8822\n",
       "0.0001  0.8882"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "def extended_features(sentence, i, history): \n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "     if i == 0:\n",
    "         features[\"prev-word\"] = \"<START>\"\n",
    "     else:\n",
    "         features[\"prev-word\"] = sentence[i-1]\n",
    "         features[\"word\"] = sentence[i]\n",
    "     return features\n",
    "\n",
    "for alpha in alphas:\n",
    "    SciKit_tagger = ScikitConsecutivePosTagger(news_train,features=extended_features, clf = BernoulliNB(alpha=alpha))\n",
    "    lst.append(round(SciKit_tagger.evaluate(news_dev_test), 4))\n",
    "df = pd.DataFrame(lst, index=alphas,columns = ['alpha'])\n",
    "df\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a way to calculate a features contribution of a feature f toward the label likelihood for a label is\n",
    "P(f|label) = count(f, label) / count(label)\n",
    "\n",
    "However, this simple approach can become problematic when a feature never occurs with a given label in the training set. In this case, our calculated value for P(f|label) will be zero, which will cause the label likelihood for the given label to be zero. Thus, the input will never be assigned this label, regardless of how well the other features fit the label.\n",
    "\n",
    "applying an aplha value will a to the probabillity and preventing this. This is know as smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: Logistic regression (5 points)\n",
    "### Part a.\n",
    "We proceed with the best feature selector from the last exercise. We will study the effect of the learner. Import *LogisticRegression* and use it with standard settings instead of *BernoulliNB*. Train on *news_train* and test on *news_dev_test* and record the result. Is it better than the best result with Naive Bayes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peder/anaconda3/envs/IN4080/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def log_scikitConsecutivePosTagger(C=1, features=extended_features):\n",
    "    SciKit_tagger = ScikitConsecutivePosTagger(news_train,\n",
    "                                           features=features, \n",
    "                                           clf = LogisticRegression(C=C, n_jobs=-1, solver='lbfgs'))\n",
    "    \n",
    "    return round(SciKit_tagger.evaluate(news_dev_test), 4)\n",
    "\n",
    "print(log_scikitConsecutivePosTagger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "Similarly to the Naive Bayes classifier, we will study the effect of smoothing. Smoothing for LogisticRegression is done by regularization. In scikit-learn, regularization is expressed by the parameter C. A smaller C means a heavier smoothing. (C is the inverse of the parameter $\\alpha$ in the lectures.) Try with C in [0.01, 0.1, 1.0, 10.0, 100.0] and see which value which yields the best result.\n",
    "\n",
    "Which C gives the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.01</td>\n",
       "      <td>0.7246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10.00</td>\n",
       "      <td>0.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100.00</td>\n",
       "      <td>0.9123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             C\n",
       "0.01    0.7246\n",
       "0.10    0.8516\n",
       "1.00    0.9032\n",
       "10.00   0.9136\n",
       "100.00  0.9123"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_ = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "lst = []\n",
    "for C in C_:\n",
    "    lst.append(log_scikitConsecutivePosTagger(C))\n",
    "df = pd.DataFrame(lst, index=C_,columns = ['C'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem comes when you have a lot of parameters (a lot of independent variables) but not too much data. In this case, the model will often fit the data almost perfectly. However because these  characteristic don't appear in future data you see, the model predicts poorly.\n",
    "\n",
    "to solve this, you add penalizes large values of the parameters by minimizing. This is also know as regularization\n",
    "\n",
    "smaller values of c specify stronger regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Features (10 points)\n",
    "### Part a.\n",
    "We will now stick to the LogistiRegression() with the optimal C from the last point and see whether we may improve the results further by extending the feature extractor with more features. First try adding a feature for the next word in the sentence, and then train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252\n"
     ]
    }
   ],
   "source": [
    "def extended_features_2(sentence, i, history): \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"word\"] = sentence[i]\n",
    "        if i < len(sentence) - 1:\n",
    "            features[\"next-word\"] = sentence[i+1]\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "print(log_scikitConsecutivePosTagger(10,extended_features_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may consider. Some candidates: is the word a number? Is it capitalized? Does it contain capitals? Does it contain a hyphen? Consider larger contexts? etc. What is the best feature set you can come up with? Train and test various feature sets and select the best one. \n",
    "\n",
    "If you use sources for finding tips about good features (like articles, web pages, NLTK code, etc.) make references to the sources and explain what you got from them.\n",
    "\n",
    "Observe that the way *ScikitConsecutivePosTagger.tag()* is written, it extracts the features from a whole sentence before it tags it. Hence it does not support  preceding tags as features. It is possible to rewrite *ScikitConsecutivePosTagger.tag()* to extract features after reading each word, and to use the *history* which keeps the preceding tags in the sentence. If you like, you may try it. However, we got surprisingly little gain from including preceding tags as features, and you are not requested to trying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_fdist = nltk.FreqDist()\n",
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "\n",
    "def extended_features_fantasy(sentence, i, history): \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:],\n",
    "                 \"prefix(1)\": sentence[i][0],\n",
    "                 \"prefix(2)\": sentence[i][:2],\n",
    "                 \"prefix(3)\": sentence[i][:3]}\n",
    "    for word in sentence:\n",
    "        for suffix in common_suffixes:\n",
    "             features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"word\"] = sentence[i]\n",
    "        features['prev-word-is-one-char']: len(sentence[i-1]) == 1\n",
    "        features['prev-word-capitalized'] = sentence[i-1].isupper(),\n",
    "        features['prev-word-is-lower'] = sentence[i-1].islower(),\n",
    "        features['prev-word-is-hyphen'] = sentence[i-1] == '-'\n",
    "        features['prev-word-is-isalpha'] = sentence[i-1].isalpha()\n",
    "        features['prev-word-is-digit'] = sentence[i-1].isdigit()\n",
    "        if i < len(sentence) - 1:\n",
    "            features[\"next-word\"] = sentence[i+1]\n",
    "            features['next-word-capitalized'] = sentence[i+1].isupper(),\n",
    "            features['next-word-is-one-char'] = len(sentence[i+1]) == 1\n",
    "            features['next-word-is-lower'] = sentence[i+1].islower(),\n",
    "            features['next-word-is-end-of-sentence'] = sentence[i+1] == '.'\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9407\n"
     ]
    }
   ],
   "source": [
    "scikit_tagger = ScikitConsecutivePosTagger(news_train,\n",
    "                                           features=extended_features_fantasy, \n",
    "                                           clf = LogisticRegression(C=10, n_jobs=-1, solver='lbfgs'))\n",
    "print(round(scikit_tagger.evaluate(news_dev_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the added features help to make a better model. I tried to extract the most important features but I could not get it to work. It would have been interesting to see what feature gave the largest contribution.\n",
    "I add prefix for sentences - that had a positive impact on the models preformance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5: Larger corpus and evaluation (10 points)\n",
    "### Part a.\n",
    "We can now test our best tagger so far on the *news_test* set. \n",
    "Do that. How is the result compared to testing on *news_dev_test*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9407\n"
     ]
    }
   ],
   "source": [
    "print(round(scikit_tagger.evaluate(news_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "But we are looking for bigger fish. How good is our settings when trained on a bigger corpus?\n",
    "\n",
    "We will use nearly the whole Brown corpus. But we will take away two categories for later evaluation: *adventure* and *hobbies*. We will also initially stay clear of *news* to be sure not to mix training and test data.\n",
    "\n",
    "Call the Brown corpus with all categories except these three for *rest*. Shuffle the tagged sentences from *rest* and strip the tags down to the original Brown tags. Then split the set into 80%-10%-10%: *rest_train*, *rest_dev_test*, *rest_test*.\n",
    "\n",
    "We can then merge these three sets with the corresponding sets from *news* to get final training and test sets:\n",
    "\n",
    "- `train = rest_train+news_train`\n",
    "- `test = rest_test + news_test`\n",
    "\n",
    "The first we should do is to establish a new baseline. Do this similarly to the way you did for the news corpus above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [cat for cat in brown.categories() if cat != 'adventure' and cat != 'hobbies' and cat!= 'news']\n",
    "rest = list(brown.tagged_sents(categories=cat))\n",
    "random.seed(2920)\n",
    "random.shuffle(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_train, rest_dev_test, rest_test = np.split(rest, [int(.8*len(rest)), int(.9*len(rest))])\n",
    "\n",
    "rest_train = originize(rest_train)\n",
    "rest_dev_test = originize(rest_dev_test)\n",
    "rest_test = originize(rest_test)\n",
    "\n",
    "train = rest_train + news_train\n",
    "test = rest_test + news_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865\n"
     ]
    }
   ],
   "source": [
    "baseline_tagger = ScikitConsecutivePosTagger(rest_train, clf = LogisticRegression(n_jobs=-1, solver='lbfgs'))\n",
    "print(round(baseline_tagger.evaluate(rest_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c.\n",
    "We can then build our tagger for this larger domain. Use the best settings from the earlier exercises, train on *train* and test on *test*. What is the accuracy of your tagger?\n",
    "\n",
    "#### Warning: Running this experiment may take 30-60 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9677\n"
     ]
    }
   ],
   "source": [
    "scikit_tagger = ScikitConsecutivePosTagger(train,\n",
    "                                           features=extended_features_fantasy, \n",
    "                                           clf = LogisticRegression(C=10, n_jobs=-1, solver='lbfgs'))\n",
    "print(round(scikit_tagger.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d.\n",
    "Test the big tagger first on *adventures* then on *hobbies*. Discuss in a few sentences why you see different results from when testing on *test*. Why do you think you got different results on *adventures* from *hobbies*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hob = brown.tagged_sents(categories='hobbies')\n",
    "hob = originize(hob)\n",
    "print(round(scikit_tagger.evaluate(hob), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9605\n"
     ]
    }
   ],
   "source": [
    "adventures = brown.tagged_sents(categories='adventure')\n",
    "adventures = originize(adventures)\n",
    "print(round(scikit_tagger.evaluate(adventures), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree of similarity between instances in the test set and those in the development set. The more similar these two datasets are, the less confident we can be that evaluation results will generalize to other datasets. For example, consider the part-of-speech tagging task. At one extreme, we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre (news):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries:\n",
    "Code. Results of the runs. Answers to the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ex6: Comparing to other taggers (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a.\n",
    "In the lectures, we spent quite some time on the HMM-tagger. NLTK comes with an HMM-tagger which we may train and test on our own corpus. It can be trained by \n",
    "\n",
    "`news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train)`\n",
    "\n",
    "and tested similarly as we have tested our other taggers. Train and test it, first on the *news* set then on the big *train*/*test* set. How does it perform compared to your best tagger? What about speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'symbols', 'states', 'transitions', 'outputs', and 'priors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5c0a77bccb47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_hmm_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddenMarkovModelTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnews_hmm_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_hmm_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'symbols', 'states', 'transitions', 'outputs', and 'priors'"
     ]
    }
   ],
   "source": [
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger()\n",
    "news_hmm_tagger.train(news_train)\n",
    "print(round(news_hmm_tagger.evaluate(news_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_hmm_tagger = nltk.HiddenMarkovModelTagger()\n",
    "news_hmm_tagger.train(train)\n",
    "print(round(news_hmm_tagger.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_hmm_tagger = HiddenMarkovModelTagger(news_train,\n",
    "#                                          features=extended_features_fantasy, \n",
    "#                                          clf = HiddenMarkovModelTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(round(news_hmm_tagger.evaluate(news_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "NLTK also comes with an averaged perceptron tagger which we may train and test. It is currently considered the best tagger included with NLTK. It can be trained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 s, sys: 35 ms, total: 33 s\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "per_tagger.train(news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9319\n"
     ]
    }
   ],
   "source": [
    "print(round(per_tagger.evaluate(news_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "per_tagger.train(train)\n",
    "print(round(per_tagger.evaluate(test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tested similarly to our other taggers. \n",
    "\n",
    "Train and test it, first on the news set and then on the big train/test set. How does it perform compared to your best tagger? Did you beat it? What about speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Code. Results of the runs. Answers to the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of mandatory assignment 2 IN4080 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
