{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080 2019, Mandatory assignment 2, part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the assignment\n",
    "**Your answer should be delivered in devilry no later than Friday, 11 October at 23:59**\n",
    "\n",
    "Mandatory assignment 2 consists of two parts \n",
    "\n",
    "* Part A on text classification (=this file)\n",
    "* Part B on tagging and sequence classification (separate document)\n",
    "    \n",
    "You should answer both parts. It is possible to get 50 points on each part, 100 points altogether. You are required to get at least 60 points to pass. It is more important that you try to answer each question than that you get it correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General requirements:\n",
    "\n",
    "\n",
    "- We assume that you have read and are familiar with IFI's requirements and guidelines for mandatory assignments\n",
    "    - https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html\n",
    "    - https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html\n",
    "- This is an individual assignment. You should not deliver joint submissions.\n",
    "- You may redeliver in Devilry before the deadline, but include all files in the last delivery. Only the last delivery will be read!\n",
    "- If you deliver more than one file, put them into a zip-archive.\n",
    "- Name your submission your_username_in4080_mandatory_2\n",
    "\n",
    "The delivery can take one of two forms:\n",
    "\n",
    "- Alternative I:\n",
    "\n",
    "    - Deliver the code files. \n",
    "    - In addition, deliver a separate pdf file containing results from the runs together with answers to the text questions.\n",
    "    \n",
    "- Alternative II:\n",
    "\n",
    "    - A jupyter notebook containing code, answers to the text questions in markup and optionally results from the runs.\n",
    "    - In addition, a pdf-version of the notebook where (in addition) all the results of the runs are included.\n",
    "\n",
    "Whether you use the first or second alternative, make sure that the code runs at the IFI machines after\n",
    "\n",
    "-\texport PATH=$PATH:/opt/ifi/python-3.7/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of part A\n",
    "In this part you will get experience with\n",
    "\n",
    "- setting up and running experiments\n",
    "- splitting your data in development and test data\n",
    "- *n*-fold cross-validation\n",
    "- evaluation and evaluation measures\n",
    "- models for text classification\n",
    "- Naive Bayes vs Logistic Regression\n",
    "- the scikit-learner toolkit\n",
    "- vectorization of categorical data\n",
    "\n",
    "As background for the current assignment you should work through two tutorials\n",
    "\n",
    "- Document classification from the NLTK book, Ch. 6. See (weekly) exercise set 3\n",
    "- The scikit-learn tutorial on text classification. See exercise set 4.\n",
    "\n",
    "If you have any questions regarding these two tutorials, we will be happy to answer them during the group/lab sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1 First classifier and *n*-fold cross-validation (10 points)\n",
    "### Part a. Inititial classifier\n",
    "We will work interactively in python/ipython/notebook. Start by importing the tools we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data we will use the Movie Reviews Corpus that comes with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import the documents similarly to how it is done in the NLTK book for the Bernoulli Naive Bayes, with one change. We there use the tokenized texts with the command\n",
    "\n",
    "- `movie_reviews.words(fileid)`\n",
    "\n",
    "Following the recipe from the scikit \"Working with text data\" page, we can instead use the raw documents which we can get from NLTK by\n",
    "\n",
    "- `movie_reviews.raw(fileid)`\n",
    "\n",
    "scikit will then tokenize for us as part of\n",
    "*count_vect.fit* (or *count_vect.fit_transform*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_movie_docs = [(movie_reviews.raw(fileid), category) for\n",
    "                   category in movie_reviews.categories() for fileid in\n",
    "                   movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will shuffle the data and split it into 200 documents for final testing (which we will not use for a while) and 1800 documents for development. Use your birth date as random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2920)\n",
    "random.shuffle(raw_movie_docs)\n",
    "movie_test = raw_movie_docs[:200]\n",
    "movie_dev  = raw_movie_docs[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the development data into 1600 documents for training and 200 for development test set, call them *train_data* and *dev_test_data*. The *train_data* should now be a list of 1600 items, where each is a pair of a text represented as a string and a label. You should then split this *train_data* into two lists, each of 1600 elements, the first, *train_texts*, containing the texts (as strings) for each document, and the *train_target*, containing the corresponding 1600 labels. Do similarly to the *dev_test_data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To be filled in\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then time to extract features from the text. We import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make a CountVectorizer *v*. This first considers the whole set of training data, to determine which features to extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer()\n",
    "v.fit(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use this vectorizer to extract features from the training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = v.transform(train_texts)\n",
    "dev_test_vectors = v.transform(dev_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what is going on, you may inspect the *train_vectors* a little more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(train_vectors, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed and see how the classifier will classify one test object, e.g.\n",
    "```\n",
    "dev_test_texts[14]\n",
    "clf.predict(dev_test_vectors[14])\n",
    "```\n",
    "We can use the procedure to predict the results for all the test_data, by\n",
    "```\n",
    "clf.predict(dev_test_vectors)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this for further evaluation (accuracy, recall, precision, etc.) by comparing to *dev_test_targets*. Alternatively, we can  get the accuracy directly by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(dev_test_vectors, dev_test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now made and tested a multinomial naive Bayes text classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "To make it easier to rerun the experiment and proceed to cross-validation, put most of exercise 1 into a procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_nb_exp(train_data, test_data):\n",
    "    \"\"\"train-data is a list of pairs, where \n",
    "      first element is representing a text\n",
    "      second element a string representing a label test-data has the same form\n",
    "   \n",
    "    Train a multinomialNB on train_data, test on test_data\n",
    "    and return the results\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the experiment from part (a) with this procedure and check that the accuracy is the same. Beware, the input should be\n",
    "\n",
    "`multi_nb_exp(train_data, dev_test_data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(acc, pred) = multi_nb_exp(train_data, dev_test_data)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c.\n",
    "Make a procedure for n-fold cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We include an option for key word arguments that are passed on to the experiment.\n",
    "\n",
    "def n_fold_kwargs(experiment, dev_data, folds=10, **kwargs):\n",
    "    \"\"\"\n",
    "    experiment is an experiment like multi_nb_exp\n",
    "    dev_data is a set of pairs \n",
    "      first element is representing a text\n",
    "      second element a string representing a label\n",
    "    **kwargs are passed on to experiment\n",
    "      \n",
    "    Run an n-fold cross-validation of experiment on\n",
    "    dev_data. return the results.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run\n",
    "\n",
    "`n_fold(multi_nb_exp, dev_data, n=9)`\n",
    "\n",
    "and calculate the accuracies for each of the 9 runs, the mean accuracy and the standard deviation of the accuracies. (In case you wonder, the reason why we are running 9-fold instead of, say 10-fold, is simply to get nice round numbers with 1800 items in our development set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Code and results of running the code as described. Answers to the questions in part C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the large variation in accuracy between the various splits, we recognize the need of using cross-validation (when we have sufficicient time):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2 Parameters of the vectorizer  (5 points)\n",
    "We have so far considered the standard parameters for the procedures from scikit-learn. These procedures have, however, many parameters. To get optimal results, we should adjust the parameters. We can go back to the split in exercise 1, and use *train_data* for training various models and *dev_test_data* for testing and comparing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the parameters for CountVectorizer we may use\n",
    "\n",
    "`help(CountVectorizer)`\n",
    "\n",
    "In ipython we may alternatively use\n",
    "\n",
    "`CountVectorizer?`\n",
    "\n",
    "We observe that *CountVectorizer* case folds by default. For a different corpus, it could be interesting to check the effect of this feature, but even  the *movie_reviews.raw()* is already in lower case, so that does not have  an effect here (You may check!) We could also have explored the effect of excahnging the default tokenizer included in CountVectorizer with the tokenized Brown corpus.\n",
    "\n",
    "Another interesting feature is *binary*. Setting this to *True* implies only counting whether a word occurs in a document and not how many times it occurs. It could be interesting to see the effect of this feature.\n",
    "\n",
    "The feature *ngram_range=[1,1]* means we use tokens  (=unigrams) only, [2,2] means that we using bigrams only, while [1,2] means both unigrams and  bigrams, and so on.\n",
    "\n",
    "Run experiments where you let *binary* vary over [False, True] and *ngram_range* vary over [[1,1], [1,2], [1,3]]. Run the experiment with 9-fold cross-validation and report the accuracy with the 6 different settings.\n",
    "\n",
    "Which settings yield the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Code and results of running the code as described. Answers to the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3  Logistic Regression (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that Logistic Regression may produce better results than Naive Bayes. We proceed with the same multinomial model for text classification (i.e. we process the data the same way and use the same vectorizer), but exchange the learner with sciki-learn's LogisticRegression. Since logistic regression is slow to train, we restrict ourselves somewhat with respect to which experiments to run. We assume that the best settings above with respect to binary and n-grams will remain the same with logistic regression as with naive Bayes. Therefore, use these best settings and run 9-fold cross-validation. What is the average accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Code and results of running the code as described. Answer to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4 The Bernoulli model (10 points)\n",
    "\n",
    "\n",
    "We will explore how the Bernoulli naive Bayes, which is used in the NLTK book for the movie data set, can be implemented in scikit-learn. We can follow the NLTK-book and extract features similarly, i.e., using\n",
    "\n",
    "`document_features(document)`\n",
    "\n",
    "to extract features from *document* into a dictionary.\n",
    "Remember that we here have to use\n",
    "\n",
    "`movie_reviews.words(fileid)`\n",
    "\n",
    "where we in exercise 1 used\n",
    "\n",
    "`movie_reviews.raw(fileid)`\n",
    "\n",
    "We can use the same splits as in exercise 1. The train_data will now be a list of 1600 items, where each is a pair where the first element is a dictionary. It remains to transform these dictionaries to numpy arrays of the form scikit accepts. For this, we can use scikit's *DictVectorizer*, see section 4.2.1 in http://scikit-learn.org/stable/modules/feature_extraction.html. (Alternatively, you can extract the features directly as arrays without using dictionaries. It is a little more work, but the\n",
    "experiments will run faster.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Make a procedure for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_exp(train_data, test_data, feature_numbs=2000):\n",
    "    \"\"\"train-data is a list of pairs, where \n",
    "      first element is a feature dictionary\n",
    "      second element a string representing a label\n",
    "    test-data has the same form\n",
    "   \n",
    "    Train a BernoulliNB on train_data, test on test_data\n",
    "    and return the result\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Combining this with exercise 2, run\n",
    "\n",
    "`n_fold(bernoulli_nb_exp, dev_data, n=9)`\n",
    "\n",
    "and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Code and results of running the code as described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5 Logistic regression (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a.\n",
    "Similarly to the Multinomial model, we can also for the Bernoulli model, combine the basic model (i.e. features and vectorization) with a logistic regression learner instead of the naive Bayes learner. Do this, and run the 9-fold cross-validation experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b.\n",
    "The default from NLTK is to use the 2000 most frequent words. We will explore the effect of the size of the feature set. Repeat the 9-fold cross-validation experiment with the 1000, 2000, 5000, 10000 and 20000 most frequent words as features, both with BernoulliNB and with LogisticRegression.\n",
    "\n",
    "#### Warning: Running this experiment may take 30-60 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Code. A 5x2 table showing the mean accuracies for 9-fold cross-validation for 1000, 2000, 5000, 1000, and 20000 features for the two different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (10 points)\n",
    "From the different classifiers with which you have experimented in this exercise set, choose the one with the best performance on the development data. Find the 200 item test set we tucked away in exercise 1. Run the best classifier on this final test set. \n",
    "\n",
    "Calculate accuracy, recall, precision and F-score for both classes.\n",
    "\n",
    "Concratulations. You have now completed a full series of experiments on text classification where you tested out various alternatives systematically on the development data before choosing the settings which gave the best results on the development set and finally testing that set-up with the whole development set as training data and the final test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliveries: \n",
    "Which classifier did you choose? The numbers asked for. Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END of Part A. Proceed to part B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
